---
title: "R Notebook"
output: html_notebook
---

This document serves to compare the data presented in Sydney's original XLSX 
file and the one generated by Sajeewa. He cleaned it up by inserting "unk" for
missing values. Because there was no file trail for this other than his own
comment, I will compare these two.

```{r setup, include = FALSE}
knitr::opts_knit$set(root.dir = PROJHOME)
```


```{r}
library('tidyverse')
library('readxl')
library('assertr')
```


## Assertations

Of course, to make sure the data are cromulent, we want to ensure they live up to
our standards of what has been reported. For this, we will use the *assertr*
package, which lets us check our data in various ways. I'm taking the data
presented in the paper thus far to create the assertions.


```{r}
size_ranges <- 
"locus\trange\tn
5-2	318-324	4
6-2	483-495	3
7-2	158-174	7
8-3	244-270	7
9-2	360-382	9
12-2	214-222	5
17-3	342-363	7
20-3	280-282	2
55-4	153-216	10
110-4	370-386	5
114-4	339-416	10
"
sr <- read_tsv(size_ranges) %>%
  separate(range, c("lower", "upper"), sep = "-") # split range into lower and upper bounds
sr
# Making the assertation string.
# To do this, we are using `sprintf()` to print the assert statement:
#   assert(within_bounds(lower, upper), `locus`)
# which means: "assert that the alleles (data) within this locus (variable) are
# within the defined range". 
# 
# The `apply()` function takes a matrix or data frame and applies a given function over each
# row (MARGIN = 1) or column (MARGIN = 2). In this case, we are specifying rows.
# 
# We then collapse all these statements with the pipe operator: %>%
# Then we add the name of the operation to the beginning
assrt_string <- apply(sr, MARGIN = 1, function(x){
    sprintf("  assert(within_bounds(%s, %s), `%s`)", x[2], x[3], x[1]) 
  }) %>% 
  paste(collapse = " %>%\n") %>%
  paste("assert_alleles_within_bounds <- .", ., sep = " %>%\n")
#
# now we can see what this statement looks like
cat(assrt_string)
# here, we evaluate it, making it available for us later on.
eval(parse(text = assrt_string)) 

YEARS <- c("2003", "2004", "2005", "2007", "2008", "2009", "2010", "2012")

STATE <- c("AU", "TS", "FR", "BL", "MX", "NE", "NY", "MN", "MI", "OR", 
           "WA", "CO", "WI", "ID", "CA", "ND")

HOST <- c("Merlot", "Pinto", "redkid", "Beryl", "Bunsi", "37", "38", 
          "11A", "cornell", "G122", "Orion", "PO7104", "PO7863", "WM31", 
          "GH", "BO7104", "Black", "Vista", "SR06233", "BL", "Fuji", "unk", 
          "Zorro", "PO7883", "Emerson", "Weihing", "Yellow")

check_data_cromulence <- . %>%
  chain_start() %>%
  verify(nrow(.) == 366) %>%          # 366 isolates
  verify(has_all_names(sr$locus)) %>% # Has all loci
  assert_alleles_within_bounds %>%    # each locus has alleles within bounds
  assert(in_set(YEARS), Year) %>%     # has all specified years
  assert(in_set(STATE), State) %>%    # has all specified States (incl. countries)
  assert(in_set(HOST), Host) %>%      # has all specified Hosts (incl. countries)
  chain_end(error_fun = warn_report)
```


## Reading in Sydney's data

Sydney's data is stored in excel format, so we have to use readxl to parse it.
First, we want to know what sheets exist.

```{r}
excel_sheets("../Analysis4 ForManu/A1_Copy of binned-genotypes_SE.xlsx")
```

We will be using the GenAlexBinned sheet. From my talks with Sydney, this sheet
contains the SSR data binned into the expected allelels. Note, this sequence of
data input and cleaning was done iteratively and what you are seeing is the 
final result.

There are always quirks with the data when it's in excel. Often, there are too
many rows, so we have to remove them with `slice()`.

Quirks specific to thse data:

 - GenAlEx format has an extra header region that should be ignored
 - The loci are formatted like so: `110-4(F)`, where the `(F)` part is not 
   informative
 - The header for the strata has extra information
 

 
```{r sydney_data}
syd <- read_excel("../Analysis4 ForManu/A1_Copy of binned-genotypes_SE.xlsx", 
                sheet = "GenAlexBinned", skip = 1) %>%
  select(-1) %>%                # removing first column, which is empty
  gather(locus, allele, -1) %>% # gather all loci into tidy columns
  mutate(locus = trimws(locus) %>% substr(1, nchar(.) - 3)) %>% # remove (F) designator
  mutate(allele = as.integer(allele)) %>% # force alleles to integers
  spread(locus, allele) %>%     # spread data out with individual loci in columns
  separate(iso_st_mcg_org_loc_yr_hst_cult_rep, 
           c("Isolate", "Severity", "MCG", "State", "Source", "Year", "Host"), 
           sep = "_") %>%
  mutate_if(is.character, trimws) %>% # Trim whitespace forom all character columns
  mutate(Severity = as.numeric(Severity)) %>%
  mutate(Source = ifelse(Source == "", "unk", Source)) %>%
  slice(-n()) %>% # remove last row
  arrange(Isolate) %>%
  check_data_cromulence
syd
```

## Reading in Sajeewa's data

These data can be read in via `reader::read_csv()`

```{r sajeewa_data}
column_specification <- cols(
  Individual = col_character(),
  iso_st_mcg_org_loc_yr_hst = col_character(),
  `5-2(F)` = col_integer(),
  `6-2(F)` = col_integer(),
  `7-2(F)` = col_integer(),
  `8-3(H)` = col_integer(),
  `9-2(F)` = col_integer(),
  `12-2(H)` = col_integer(),
  `17-3(H)` = col_integer(),
  `20-3(F)` = col_integer(),
  `55-4(F)` = col_integer(),
  `110-4(H)` = col_integer(),
  `114-4(H)` = col_integer()
)
saj <- read_csv("../Analysis4 ForManu/A2_Copy4 EUR_AUS_forManu.csv", skip = 2, col_types = column_specification) %>%
  select(-1) %>%
  gather(locus, allele, -1) %>% # gather all loci into tidy columns
  mutate(locus = trimws(locus) %>% substr(1, nchar(.) - 3)) %>% # remove (F) designator
  spread(locus, allele) %>%     # spread data out with individual loci in columns
  separate(iso_st_mcg_org_loc_yr_hst, # Note: this corresponds closer to the initial data
           c("Isolate", "Severity", "MCG", "State", "Source", "Year", "Host"), 
           sep = "_") %>%
  mutate_if(is.character, trimws) %>% # Trim whitespace forom all character columns
  mutate(Severity = as.numeric(Severity)) %>%
  arrange(Isolate) %>%
  check_data_cromulence
saj
```

## Comparison


We've sorted each data set by Isolate, so that means that we should expect them
to have the same data. The `setequal()` checks whether or not both data sets
have the same rows (in any order)

```{r, equal_sets}
setequal(saj, syd)
```

Okay, something's not cromulent here. We'll have to manaully inspect these:

```{r}
syddiff <- setdiff(syd, saj)
sajdiff <- setdiff(saj, syd)
the_difference <- bind_rows(syd = syddiff, saj = sajdiff, .id = "source") %>% arrange(Isolate)
head(the_difference, n = 20)
```

It appears that there are a couple of discrepancies:

1. Isolate 805 is flagged for some reason in Sydney's data set
2. Tasmania (TS, Sydney) has been changed to Australia (AU, Sajeewa)
3. Belgium (BL, Sydney) has been changed to France (FR, Sajeewa)


## Session Information

```{r session_info}
options(width = 100)
devtools::session_info()
```

